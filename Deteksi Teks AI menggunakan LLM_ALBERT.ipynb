{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8592674,"sourceType":"datasetVersion","datasetId":5139969},{"sourceId":8602321,"sourceType":"datasetVersion","datasetId":5146956},{"sourceId":8608988,"sourceType":"datasetVersion","datasetId":5151593}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T12:39:19.693960Z","iopub.execute_input":"2024-06-13T12:39:19.694251Z","iopub.status.idle":"2024-06-13T12:39:20.835202Z","shell.execute_reply.started":"2024-06-13T12:39:19.694225Z","shell.execute_reply":"2024-06-13T12:39:20.834057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from multiprocessing import cpu_count\nn_cores = cpu_count()\nprint(f'Number of Logical CPU cores: {n_cores}')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:39:46.629550Z","iopub.execute_input":"2024-06-13T12:39:46.630047Z","iopub.status.idle":"2024-06-13T12:39:46.638410Z","shell.execute_reply.started":"2024-06-13T12:39:46.630016Z","shell.execute_reply":"2024-06-13T12:39:46.637536Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import psutil\n\n# Mendapatkan total memori (RAM)\ntotal_memory = psutil.virtual_memory().total\n\n# Mengonversi dari byte ke gigabyte\ntotal_memory_gb = total_memory / (1024 ** 3)\nprint(f\"Total RAM: {total_memory_gb:.2f} GB\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:39:49.053018Z","iopub.execute_input":"2024-06-13T12:39:49.053758Z","iopub.status.idle":"2024-06-13T12:39:49.059336Z","shell.execute_reply.started":"2024-06-13T12:39:49.053727Z","shell.execute_reply":"2024-06-13T12:39:49.058381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.corpus import stopwords\nimport nltk\n\n# Pastikan Anda telah mengunduh stopwords NLTK\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:39:51.581122Z","iopub.execute_input":"2024-06-13T12:39:51.582162Z","iopub.status.idle":"2024-06-13T12:39:52.790200Z","shell.execute_reply.started":"2024-06-13T12:39:51.582127Z","shell.execute_reply":"2024-06-13T12:39:52.789347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers datasets scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:39:56.068175Z","iopub.execute_input":"2024-06-13T12:39:56.068859Z","iopub.status.idle":"2024-06-13T12:40:09.449974Z","shell.execute_reply.started":"2024-06-13T12:39:56.068827Z","shell.execute_reply":"2024-06-13T12:40:09.448921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install accelerate torch","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:40:13.787758Z","iopub.execute_input":"2024-06-13T12:40:13.788127Z","iopub.status.idle":"2024-06-13T12:40:25.896132Z","shell.execute_reply.started":"2024-06-13T12:40:13.788095Z","shell.execute_reply":"2024-06-13T12:40:25.895188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install transformers[torch]","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:40:29.961886Z","iopub.execute_input":"2024-06-13T12:40:29.962230Z","iopub.status.idle":"2024-06-13T12:40:42.034619Z","shell.execute_reply.started":"2024-06-13T12:40:29.962198Z","shell.execute_reply":"2024-06-13T12:40:42.033503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport transformers\nimport accelerate\n\nprint(torch.__version__)\nprint(transformers.__version__)\nprint(accelerate.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:40:46.760490Z","iopub.execute_input":"2024-06-13T12:40:46.761177Z","iopub.status.idle":"2024-06-13T12:40:52.361475Z","shell.execute_reply.started":"2024-06-13T12:40:46.761145Z","shell.execute_reply":"2024-06-13T12:40:52.360582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom datasets import Dataset\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:41:37.071386Z","iopub.execute_input":"2024-06-13T12:41:37.071959Z","iopub.status.idle":"2024-06-13T12:41:50.306425Z","shell.execute_reply.started":"2024-06-13T12:41:37.071928Z","shell.execute_reply":"2024-06-13T12:41:50.305425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path = \"/kaggle/input/training-and-validation\"\n# Menggabungkan path dengan nama file\nfile_path = os.path.join(path, 'training.json')\n\n# Membaca JSON newline-delimited dari StringIO object\ndf = pd.read_json(file_path, lines=True)\n\n# Menampilkan beberapa baris pertama dari DataFrame\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:41:58.590223Z","iopub.execute_input":"2024-06-13T12:41:58.591047Z","iopub.status.idle":"2024-06-13T12:41:58.672591Z","shell.execute_reply.started":"2024-06-13T12:41:58.591011Z","shell.execute_reply":"2024-06-13T12:41:58.671726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_gram_range = (3, 4)\nvectorizer = CountVectorizer(ngram_range=n_gram_range)\n\n# Transformasikan teks menjadi fitur n-gram\nX = vectorizer.fit_transform(df['text'])\n\n# Mendapatkan frekuensi n-gram\nn_gram_frequencies = X.sum(axis=0).A1\nn_gram_features = vectorizer.get_feature_names_out()\n\n# Buat DataFrame dari frekuensi n-gram\nn_gram_df = pd.DataFrame({'n_gram': n_gram_features, 'frequency': n_gram_frequencies})\n\n# Pilih top-k n-gram berdasarkan frekuensi\ntop_k = 10\ntop_k_n_grams = n_gram_df.nlargest(top_k, 'frequency')\n\n# Tampilkan hasil\nprint(top_k_n_grams)\n\n# List dari top-k n-gram\ntop_k_n_gram_list = top_k_n_grams['n_gram'].tolist()\nprint(\"Top-k n-grams:\", top_k_n_gram_list)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:44:18.098081Z","iopub.execute_input":"2024-06-13T12:44:18.098815Z","iopub.status.idle":"2024-06-13T12:44:24.023552Z","shell.execute_reply.started":"2024-06-13T12:44:18.098782Z","shell.execute_reply":"2024-06-13T12:44:24.022609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom datasets import Dataset\nfrom transformers import AlbertTokenizer, AlbertForSequenceClassification, TrainingArguments, Trainer\n\n# 1. Split the dataset\ntrain_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n\n# Convert DataFrame to Hugging Face Dataset\ntrain_dataset = Dataset.from_pandas(train_df[['text', 'label']])\ntest_dataset = Dataset.from_pandas(test_df[['text', 'label']])\n\n# 2. Load tokenizer and model\ntokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\nmodel = AlbertForSequenceClassification.from_pretrained('albert-base-v2', num_labels=2)  # Adjust num_labels for your task\n\n# Tokenize the data\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding='max_length', truncation=True)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# 3. Fine-tune the model\ntraining_args = TrainingArguments(\n#     fp16=False,  # Disable FP16 mixed precision training\n#     fp16_full_eval=False, # Disable FP16 evaluation\n    \n    output_dir='./results',                  # Direktori output\n    evaluation_strategy=\"epoch\",             # Evaluasi setiap selesai satu epoch\n    per_device_train_batch_size=16,          # Ukuran batch untuk pelatihan\n    per_device_eval_batch_size=16,           # Ukuran batch untuk evaluasi\n    num_train_epochs=3,                      # Jumlah epoch pelatihan\n    weight_decay=0.01,                       # Besarnya weight decay\n    logging_dir='./logs',                    # Direktori untuk menyimpan log\n    logging_steps=10,                        # Log setiap 10 langkah\n    fp16=True,                               # Menggunakan mixed precision training\n    gradient_accumulation_steps=2,           # Menggunakan akumulasi gradient untuk batch size yang lebih besar\n    learning_rate=2e-5,                      # Learning rate\n    lr_scheduler_type='linear',              # Scheduler learning rate\n)\n\n# Check if a GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\n# Modify the Trainer instantiation to use the GPU\ntrainer = Trainer(\n    model=model,                         # The instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # Training arguments, defined above\n    train_dataset=train_dataset,         # Training dataset\n    eval_dataset=test_dataset,           # Evaluation dataset\n    tokenizer=tokenizer\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:45:09.414728Z","iopub.execute_input":"2024-06-13T12:45:09.415347Z","iopub.status.idle":"2024-06-13T12:45:29.060550Z","shell.execute_reply.started":"2024-06-13T12:45:09.415317Z","shell.execute_reply":"2024-06-13T12:45:29.059587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T12:45:33.292728Z","iopub.execute_input":"2024-06-13T12:45:33.293143Z","iopub.status.idle":"2024-06-13T13:27:48.527551Z","shell.execute_reply.started":"2024-06-13T12:45:33.293113Z","shell.execute_reply":"2024-06-13T13:27:48.525912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 4. Evaluate the model\npredictions = trainer.predict(test_dataset)\npreds = predictions.predictions.argmax(-1)\nlabels = predictions.label_ids\n\naccuracy = accuracy_score(labels, preds)\nprint(f'Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:27:53.669109Z","iopub.execute_input":"2024-06-13T13:27:53.669466Z","iopub.status.idle":"2024-06-13T13:28:28.823088Z","shell.execute_reply.started":"2024-06-13T13:27:53.669436Z","shell.execute_reply":"2024-06-13T13:28:28.821896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_save_path = '/kaggle/working/alta2023/'\ntrainer.save_model(model_save_path)\ntokenizer.save_pretrained(model_save_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:28:58.780379Z","iopub.execute_input":"2024-06-13T13:28:58.781419Z","iopub.status.idle":"2024-06-13T13:28:58.872604Z","shell.execute_reply.started":"2024-06-13T13:28:58.781383Z","shell.execute_reply":"2024-06-13T13:28:58.871710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tentukan direktori model yang disimpan\nmodel_save_path = '/kaggle/working/alta2023/'\n \n# 2. Load tokenizer and model\ntokenizer_saved = AlbertTokenizer.from_pretrained(model_save_path)\nmodel_saved = AlbertForSequenceClassification.from_pretrained(model_save_path, num_labels=2)  # Adjust num_labels for your task\n\n# Menggunakan model dan tokenizer\ntext = \"Assistant Attorney General Oberdorfer presented the case on behalf of the United States. He was joined on the briefs by former Solicitor General Rankin, Solicitor General Cox, and Harry Baum.\"\n\n# Preprocess input text\ninputs = tokenizer_saved(text, return_tensors='pt')\n\n# Memberikan input yang telah dipreprocessing ke model\noutputs = model_saved(**inputs)\n\n# Mendapatkan logits dan prediksi\nlogits = outputs.logits\npredictions = logits.argmax(dim=-1)\n\n# Menampilkan prediksi\nprint(f\"Prediksi: {predictions.item()}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:29:03.550094Z","iopub.execute_input":"2024-06-13T13:29:03.550437Z","iopub.status.idle":"2024-06-13T13:29:03.878775Z","shell.execute_reply.started":"2024-06-13T13:29:03.550413Z","shell.execute_reply":"2024-06-13T13:29:03.877739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = \"/kaggle/input/test-alta2023\"\n# Menggabungkan path dengan nama file\ntest_file = os.path.join(test_path, 'test_data.json')\n\n# Membaca JSON newline-delimited dari StringIO object\ndf_test = pd.read_json(test_file, lines=True)\n\n# Menampilkan beberapa baris pertama dari DataFrame\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:29:09.041507Z","iopub.execute_input":"2024-06-13T13:29:09.042344Z","iopub.status.idle":"2024-06-13T13:29:09.063625Z","shell.execute_reply.started":"2024-06-13T13:29:09.042311Z","shell.execute_reply":"2024-06-13T13:29:09.062685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_prediction(text):\n    # Preprocess input text\n    inputs = tokenizer_saved(text, return_tensors='pt')\n    \n    # Memberikan input yang telah dipreprocessing ke model\n    outputs = model_saved(**inputs)\n    \n    # Mendapatkan logits dan prediksi\n    logits = outputs.logits\n    prediction = logits.argmax(dim=-1).item()\n    \n    return prediction\n\n# Apply the function to the text column and create a new label column\ndf_test['label'] = df_test['text'].apply(get_prediction)\n\n# Menampilkan DataFrame dengan kolom label baru\nprint(df_test)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:29:15.664272Z","iopub.execute_input":"2024-06-13T13:29:15.664626Z","iopub.status.idle":"2024-06-13T13:32:08.699795Z","shell.execute_reply.started":"2024-06-13T13:29:15.664597Z","shell.execute_reply":"2024-06-13T13:32:08.698714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test_1 = df_test[['id', 'label']].copy()\ndf_test_1.head()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:32:20.426920Z","iopub.execute_input":"2024-06-13T13:32:20.427312Z","iopub.status.idle":"2024-06-13T13:32:20.441855Z","shell.execute_reply.started":"2024-06-13T13:32:20.427285Z","shell.execute_reply":"2024-06-13T13:32:20.440680Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:32:31.086067Z","iopub.execute_input":"2024-06-13T13:32:31.086422Z","iopub.status.idle":"2024-06-13T13:32:31.091551Z","shell.execute_reply.started":"2024-06-13T13:32:31.086394Z","shell.execute_reply":"2024-06-13T13:32:31.090649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Mengkonversi DataFrame ke format dictionary\njson_lines = df_test_1.to_dict(orient='records')\n\n# Menentukan jalur file output\noutput_file_path = '/kaggle/working/answer_tanpa_stopword_removal.json'\n\ntry:\n    # Menulis data ke file dalam format JSON lines\n    with open(output_file_path, 'w') as f:\n        for item in json_lines:\n            json.dump(item, f)\n            f.write('\\n')\n    print(f\"Data telah disimpan ke {output_file_path}\")\nexcept Exception as e:\n    print(f\"Terjadi kesalahan saat menulis file: {e}\")\n\n# Verifikasi bahwa file telah berhasil disimpan\nimport os\n\nif os.path.exists(output_file_path):\n    print(f\"File {output_file_path} berhasil dibuat.\")\nelse:\n    print(f\"File {output_file_path} tidak ditemukan.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:32:36.918424Z","iopub.execute_input":"2024-06-13T13:32:36.918816Z","iopub.status.idle":"2024-06-13T13:32:36.962584Z","shell.execute_reply.started":"2024-06-13T13:32:36.918787Z","shell.execute_reply":"2024-06-13T13:32:36.961554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path1 = \"/kaggle/working\"\n# Menggabungkan path dengan nama file\nfile_path1 = os.path.join(path1, 'answer_tanpa_stopword_removal.json')\n\n# Membaca JSON newline-delimited dari StringIO object\ndf1 = pd.read_json(file_path1, lines=True)\ndf1","metadata":{"execution":{"iopub.status.busy":"2024-06-13T13:32:53.456962Z","iopub.execute_input":"2024-06-13T13:32:53.457321Z","iopub.status.idle":"2024-06-13T13:32:53.481142Z","shell.execute_reply.started":"2024-06-13T13:32:53.457293Z","shell.execute_reply":"2024-06-13T13:32:53.480148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path2 = \"/kaggle/input/sample\"\n# Menggabungkan path dengan nama file\nfile_path2 = os.path.join(path2, 'validation_sample_output.json')\n\n# Membaca JSON newline-delimited dari StringIO object\ndf2 = pd.read_json(file_path2, lines=True)\ndf2","metadata":{"execution":{"iopub.status.busy":"2024-06-05T04:43:06.515924Z","iopub.execute_input":"2024-06-05T04:43:06.516662Z","iopub.status.idle":"2024-06-05T04:43:06.541976Z","shell.execute_reply.started":"2024-06-05T04:43:06.516617Z","shell.execute_reply":"2024-06-05T04:43:06.540931Z"},"trusted":true},"execution_count":null,"outputs":[]}]}